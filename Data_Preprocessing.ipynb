{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Preprocessing your data is critical to ensuring that your machine learning method works properly, and is shoould be done at near the beginning of any data analysis. Consider backpacking in some remote woods for a few days. If you just set out, you'll most likely find when you are hungry, you'll have to figure out what nature has in her pantry. When you're thirsty, say hello to giardia as you drink water form a nearby stream. Or you could take some time do perform the boring: preparation. *That's what data preprocessing to machine learning.*\n",
    "\n",
    "##### Doing the boring part means we can have more fun later.\n",
    "\n",
    "Here, we will explore the essentials for preparing any dataset for any machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "### What is a library?\n",
    "\n",
    "A library, in Python, is a tool that used for a specific job. Giving the library inputs allows it to do work so you don't have to. It will then give you the outputs you need in orer to complete your task. For this project we will rely on three primary libraries that will optimize our data for making efficient machine learning models.\n",
    "\n",
    "The three libraries are:\n",
    "+ Numpy: includes mathematical tools we'll need\n",
    "+ Pandas: used for importing and managing data sets\n",
    "+ Matplotlib: includes useful tools for plotting data; we'll primarily use a sub-library, called pyplot\n",
    "\n",
    "*Let's begin by importing these libraries:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np  # the \"as np\" aliases numpy so that we can call it by using \"np\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plots our figures inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Of course we cannot process any data, let alone answer any data science question, without data. To import data, we should know in what directory our data are located. If you have cloned this repository, then data are found in the *data/* directory. Once we know where our data are located, we will use the pandas library import them into our work space. It is best to save the data as a variable so we may access the data more efficiently as we move through any analysis or model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data set as a pandas dataframe: data_df\n",
    "data_df = pd.read_csv('data/Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code saves our data in a dataframe object called *data_df*. It is highly recommended that you look at your data immediately after importing it. We will look at the first and last few rows of the dataframe, and its shape to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first rows of the dataframe\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the last rows of the dataframe\n",
    "data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the shape of the dataframe (rows = observattions, columns = fields)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, of course, a number of other things to do with your data, all more or less important for performing you initial exploratory data analysis (EDA), but our focus here is on preprocessing the data for ingesting directly into a machine learning method.\n",
    "\n",
    "### Differentiate between features and independent variables\n",
    "\n",
    "For this example, we want to process data for a machine learning model that will help us predict whether or not a customer will purchase a product. The dependent variable, or response, $y$, is will the customer purchase or not. This is reflected in the last column of our data set. Our machine learing model will base its decision on the independent variables, or features, $x_{n}$, in the data set. The features are the customer's *country*, their *age*, and their *salary*.\n",
    "\n",
    "As we can see from the above peeks into the data, $x_{1}=Country$, $x_{2}=Age$, and $x_{3}=Salary$, while $y=Purchased$. We will want to separate these into matrices $\\textbf{X}$ and $\\textbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]] \n",
      " y: \n",
      " ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# take lines of the dataset from the features: X\n",
    "X = data_df.iloc[:, :-1].values\n",
    "\n",
    "# take lines of the dataset from the response: y (the last column)\n",
    "y = data_df.iloc[:, -1].values\n",
    "\n",
    "# view the separated data\n",
    "print('X:', '\\n', X, '\\n', 'y:', '\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing The Data\n",
    "\n",
    "Data, in real life, almost always has issues with it. One common issue is missing data. Python will often show missing data with 'NaN'. Whhat can we do with this missing data? There are some options available, but we should always know why to use that option. For example, we see, above, that we have some missing data in our dataframe. We could just remove the entire observation since there is something missing. This can be very dangerous, as removing data can skew you analysis and models. Other options come in the form of imputation. This can mean replacing missing data with values like the mean, or median, of the other values in that field. We won't go into when to use what method here, but *you should always know why you're handling missing data the way you are.*\n",
    "\n",
    "With our dataset here, we have a missing value in the Age and Salary fields each. As these are numeric, and don't have any immediate eye-catching outlier, we'll just impute the missing values with the mean of the other values in the respective fields. So as to not reinvent the wheel, there is a library that already is able to impute data: Scikit-Learn. We will import this library and impute the values with mean, as we already said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Imputer class scikit-learn sub-library, preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# create class object\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)  # mean is default strategy, typed for education\n",
    "\n",
    "# fit the imputer\n",
    "imputer = imputer.fit(X[:, 1:3])  # fit to Age and Salary columns\n",
    "\n",
    "# handling missing values in data\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing values have been imputed, let's look at the data to see if it was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "\n",
    "Our dataset has two categorical variables: Country and Purchased. Since the values in the fields are categories, they are categorical. Go figure. Machine learnng methods are mathematical models and therefore require that the inputs be numerical. This requires us to encode these categorical fields with numbers. We will do this by again calling on Scikit Learn, but instead of the Imputer class, we use the LabelEncoder class. LabelEncoder will convert our variable for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LabelEncoder class from sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create the label encoder object\n",
    "labelencoder_X = LabelEncoder()\n",
    "\n",
    "# fit the label encoder object to our data\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])  # fit to Country column and transform the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were the values encoded correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 44.0 72000.0]\n",
      " [2 27.0 48000.0]\n",
      " [1 30.0 54000.0]\n",
      " [2 38.0 61000.0]\n",
      " [1 40.0 63777.77777777778]\n",
      " [0 35.0 58000.0]\n",
      " [2 38.77777777777778 52000.0]\n",
      " [0 48.0 79000.0]\n",
      " [1 50.0 83000.0]\n",
      " [0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They were, indeed, encoded, but there is an issue here: since machine learnng models are mathematically based, the numbers corresponding to each country have the same properties as numbers. In this case, since 2 > 1 > 0, mathematically, Spain > Germany > France. Disregarding your world view, this is inaccurate since there is no relational value here. We cannot say that Spain has a higher value than France, but machine learning models will! So we will introduce *dummy variables* to retain the categorical properties in the machine learning model.\n",
    "\n",
    "This means instead of having a single column for Country with $ 0 = France $, and so forth, we will have *three* columns corresponding to each individual country, France Germany and Spain, respectively, where the values in the columns are '1' if the observation is for that column's country, and '0' otherwise.\n",
    "\n",
    "To create these dummy variables, we will use another class from the preprocessing sub-library, called OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OneHotEncoder class from sklearn.preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# create OneHotEncoder object\n",
    "onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "\n",
    "# fit to X\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2          3             4\n",
      "0  1.0  0.0  0.0  44.000000  72000.000000\n",
      "1  0.0  0.0  1.0  27.000000  48000.000000\n",
      "2  0.0  1.0  0.0  30.000000  54000.000000\n",
      "3  0.0  0.0  1.0  38.000000  61000.000000\n",
      "4  0.0  1.0  0.0  40.000000  63777.777778\n",
      "5  1.0  0.0  0.0  35.000000  58000.000000\n",
      "6  0.0  0.0  1.0  38.777778  52000.000000\n",
      "7  1.0  0.0  0.0  48.000000  79000.000000\n",
      "8  0.0  1.0  0.0  50.000000  83000.000000\n",
      "9  1.0  0.0  0.0  37.000000  67000.000000\n"
     ]
    }
   ],
   "source": [
    "# convert X to a dataframe just for readability, not for use!\n",
    "print(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now our machine learning models will treat this categorical data appropriately. The last categorical field we have, Purchased, is binary: either they purchased or they didn't. This means we will not need One Hot Encoding to create the dummy variables like we did with the Country field. We can just use LabelEncoder and be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the LabelEncoder class (make an object)\n",
    "labelencoder_y = LabelEncoder()\n",
    "\n",
    "# fit the object to our Purchase column and encode the values\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome sauce! We have encoded our categorical variables! The dummy variable issue is very important when considering multiple linear regression models as well as multiple logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Into Training and Testing Sets\n",
    "\n",
    "Our dataset is all the data we have. We cannot train our machine learning models with the entire data set becuase we will then have no way of knowing if the model will deliver an accurate response given a previously unknown observation. This means we need to split our data set into two sets: training and testing. The training set will be used, obviously, for training our machne learning models, while the test set set (you guessed it!) will be used to test the model and determine its performance.\n",
    "\n",
    "To do this, we will make use of the model_selection sub-library from Scikit Learn, which has a class called train_test_split. This class will take our data set and perform the split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split class from cklearn.cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create the the train and test set and define them at the same time, with 20% in the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our train and test sets. Note that we expect, since we had 10 observations, we should have 8 observations in the train set and 2 in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training Set) \n",
      "      0    1    2          3             4\n",
      "0  0.0  1.0  0.0  40.000000  63777.777778\n",
      "1  1.0  0.0  0.0  37.000000  67000.000000\n",
      "2  0.0  0.0  1.0  27.000000  48000.000000\n",
      "3  0.0  0.0  1.0  38.777778  52000.000000\n",
      "4  1.0  0.0  0.0  48.000000  79000.000000\n",
      "5  0.0  0.0  1.0  38.000000  61000.000000\n",
      "6  1.0  0.0  0.0  44.000000  72000.000000\n",
      "7  1.0  0.0  0.0  35.000000  58000.000000  \n",
      " Y Training Set \n",
      "    0\n",
      "0  1\n",
      "1  1\n",
      "2  1\n",
      "3  0\n",
      "4  1\n",
      "5  0\n",
      "6  0\n",
      "7  1\n"
     ]
    }
   ],
   "source": [
    "print('X Training Set) \\n', pd.DataFrame(X_train), ' \\n', 'Y Training Set \\n', pd.DataFrame(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Test Set) \n",
      "      0    1    2     3        4\n",
      "0  0.0  1.0  0.0  30.0  54000.0\n",
      "1  0.0  1.0  0.0  50.0  83000.0  \n",
      " Y Test Set \n",
      "    0\n",
      "0  0\n",
      "1  0\n"
     ]
    }
   ],
   "source": [
    "print('X Test Set) \\n', pd.DataFrame(X_test), ' \\n', 'Y Test Set \\n', pd.DataFrame(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, boy! We are almost there! Note that we expect our machine learning model can learn from the training set, and apply what it learned to the test set. We will be able to see how well the model learned the correlations from the trainnig set because we will know the actual outcomes (whether a customer purchased) in the test set. If the model learned the correlations from the training set *too well* without understanding the logic behind those correlations, then we will encounter a situation called *over-fitting* our model, and the model will not make good predictions. We can address this with regularization, but not in this project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling is necessary for machine learning becuase features in a dataset are not in the same scale. If you look at our data, above, the Salary values are three orders of magnitude higher than the Age values. Machine learning methods use the Euclidean distance between points:\n",
    "\n",
    "$\n",
    "d = \\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}\n",
    "$,\n",
    "\n",
    "which means, in the case of Salary and Age, the squared difference in Salary will be *six* orders of magnitude higher, and will, therefore, dominate the Euclidean distance calculation and will not push out the Age feature.\n",
    "\n",
    "So we will need to make our values the same scale. This can be accomplished in many ways, but often it is done with *standardization*, or *normalization*, where:\n",
    "\n",
    "$\n",
    "x_{stand}=\\frac{x-mean(x)}{S.D.(x)},\n",
    "$\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "x_{norm}=\\frac{x-min(x)}{max(x)-min(x)}.\n",
    "$\n",
    "\n",
    "These methods will ensure that no feature dominates another.\n",
    "\n",
    "We can do this with StandardScaler class from Scikit sub-library, preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import StandardScalar from sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# instantiate the object\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "# fit and transform the training set\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "\n",
    "# only need to transform the test set because it is already fit to the training set\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, do we need to fit and transform the dummy variables?\n",
    "\n",
    "Well, if you Google it, some say yes, and others say no. Realistically, it depends. It won't break your model if you don't scale them, but if you wish to keep some interpretation in your model, as in, if we scale them, good, we retain scale for everything and good for predictions, but we lose interpretation as to which observations correspond to which Country. Since we have no interpretation to make for this project, we've scaled them.\n",
    "\n",
    "Let's look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train Set\n",
      " [[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]] \n",
      " X Test Set\n",
      " [[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297]\n",
      " [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]\n"
     ]
    }
   ],
   "source": [
    "print('X Train Set\\n', X_train, '\\n', 'X Test Set\\n', X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that all the values are on the same scale. Success, yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Should we apply feature scaling to the response variable?\n",
    "\n",
    "Nope.\n",
    "\n",
    "Since this is a categorical response variable, the problem is of the *classification* type. If we were dealing with a regression that intakes a large number of values for the dependent variable (i.e.: more than just the '0' and '1' presented in this example), we will need to ensure we perform feature scaling on *y*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey-dokey. Now stand up and do a little dance, because your data has been preprocessed and that high you get from doing machine learning is ready to happen. Yeehaw!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
